{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the sample\n",
    "def gen_data():\n",
    "    nobs = 1000\n",
    "    beta = 3\n",
    "    x = np.random.uniform(low=-3., high=3., size=nobs)\n",
    "    e = np.random.normal(loc=0.0, scale=1.0, size=nobs)\n",
    "    y = 1*(1 + beta * x + e >= 0) \n",
    "    return y,x,nobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "[[3.87069548e-29 8.79148501e-30]\n",
      " [8.79148501e-30 1.99680417e-30]]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def ndVuong(model1,model2,alpha,rs,S_cv):\n",
    "    \n",
    "    model1_fit = model1.fit(disp=False)\n",
    "    ll1 = model1.loglikeobs(model1_fit.params)\n",
    "    grad1 =  model1.score(model1_fit.params)\n",
    "    hess1 =  model1.hessian(model1_fit.params)\n",
    "    print(hess1.shape)\n",
    "    \n",
    "    model2_fit = model2.fit(disp=False)\n",
    "    ll2 = model2.loglikeobs(model2_fit.params)\n",
    "    grad2 =  model2.score(model2_fit.params)\n",
    "    hess2 =  model2.hessian(model2_fit.params)\n",
    "    \n",
    "    k1 = len(model1_fit.params)\n",
    "    k2 = len(model2_fit.params)\n",
    "    k = k1 + k2\n",
    "    n = len(ll1)\n",
    "    \n",
    "    #A_hat:\n",
    "    A_hat1 = np.concatenate([hess1,np.zeros((k1,k2))])\n",
    "    A_hat2 = np.concatenate([np.zeros((k2,k1)),-1*hess2])\n",
    "    A_hat = np.concatenate([A_hat1,A_hat2],axis=1)\n",
    "    \n",
    "    #B_hat, covariance of the score...\n",
    "    B_hat =  np.cov([grad1,grad2]) #might be a mistake here..\n",
    "    \n",
    "\n",
    "    #W_hat = (sqrtm(B_hat)/A_hat)*sqrtm(B_hat);\n",
    "    #[~,V]=eig(W_hat);\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "yn,xn,nobs = gen_data()\n",
    "model1 = sm.Probit(yn,sm.add_constant(xn))\n",
    "model2 = sm.Logit(yn,sm.add_constant(xn))\n",
    "\n",
    "print(ndVuong(model1,model2,.05,None,None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "    #B_hat:\n",
    "    meandL = mean([d_logf,-d_logg]);\n",
    "    dmeandL = [d_logf,-d_logg] - repmat(meandL,n,1);\n",
    "    B_hat =\n",
    "\n",
    "    W_hat = (sqrtm(B_hat)/A_hat)*sqrtm(B_hat);\n",
    "    [~,V]=eig(W_hat);\n",
    "\n",
    "\n",
    "    vecV = diag(V);\n",
    "    abs_vecV = abs(vecV)-max(abs(vecV));\n",
    "    rho_star = 1*(abs_vecV==0);\n",
    "    rnorm = rho_star'*rho_star;\n",
    "    rho_star = sqrt(rnorm)^(-1)*rho_star;\n",
    "\n",
    "    Z0 = randn(rs,S_cv,k+1);\n",
    "\n",
    "    VZ = [1,rho_star';rho_star,eye(k)];\n",
    "\n",
    "    Z = Z0*sqrtm(VZ+10^(-12)*eye(k+1));\n",
    "\n",
    "    Z_L = Z(:,1);            #$Z_Lambda$\n",
    "    Z_p = Z(:,2:k+1);        #$Z_phi^\\ast$\n",
    "\n",
    "    #trace(V)  #diagonostic line\n",
    "\n",
    "    trVsq = trace(V^2);\n",
    "    Vnmlzd = V/sqrt(trVsq);   #V, normalized by sqrt(trVsq);\n",
    "\n",
    "    J_Lmod = @(sig,c)sig*Z_L - (Z_p.^2)*diag(Vnmlzd)/2+ trace(Vnmlzd)/2;\n",
    "\n",
    "    J_omod = @(sig,c)sig^2 - 2*sig*Z_p*Vnmlzd*rho_star + ...\n",
    "                                           (Z_p.^2)*(diag(Vnmlzd).^2) + c;\n",
    "\n",
    "    quant = @(sig,c)quantile(abs(J_Lmod(sig,c)./sqrt(max(1e-10,J_omod(sig,c)))),1-alpha);\n",
    "\n",
    "                                      # quantile as a function of sigma and c.\n",
    "                                      # the lower bound 1e-10 is used so that\n",
    "                                      # inf does not appear as a value of\n",
    "                                      # quant; doing so ensures stability of\n",
    "                                      # the solution.\n",
    "\n",
    "    sigstar = @(c)fminbnd(@(sig)-quant(sig,c),0,5);\n",
    "\n",
    "    cv0 = quant(sigstar(0),0);        # critical value with c=0\n",
    "\n",
    "    z_normal = norminv(1-alpha/2);\n",
    "    z_nor_sim = max(z_normal,quantile(abs(Z_L),1-alpha)); #simulated z_normal\n",
    "\n",
    "    if cv0 - z_nor_sim <= 0.1;  # if critical value with c=0 is not very big\n",
    "\n",
    "        cv = max(cv0,z_normal);                     # use c = 0\n",
    "\n",
    "        cstar = 0;\n",
    "\n",
    "    else                              # otherwise, increase c.\n",
    "\n",
    "        Func = @(c)((quant(sigstar(c),c)-z_nor_sim)-0.1)^2;\n",
    "        cstar = fminbnd(Func,0,10);\n",
    "        cv = max(quant(sigstar(cstar),cstar),z_normal);\n",
    "    end\n",
    "\n",
    "    #Computing the ND test statistic:\n",
    "    nLR_hat = sum(logf-logg);         #sample log-likelihood ratio times n\n",
    "\n",
    "    nomega2_hat = (logf-logg)'*(logf-logg) - nLR_hat^2/n;\n",
    "                                          #n times sample variance of \\Lambda_i    \n",
    "    #Non-degenerate Vuong Tests    \n",
    "    Tnd = (nLR_hat+trace(V)/2)/sqrt(nomega2_hat + cstar*trace(V.^2));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
