{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#math\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.linalg as linalg\n",
    "import scipy.special\n",
    "#graphing\n",
    "import matplotlib.pyplot as plt\n",
    "#stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.base.model import GenericLikelihoodModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000, 2)\n",
      "ModeResult(mode=array([0.]), count=array([78]))\n"
     ]
    }
   ],
   "source": [
    "beta0 = 1.\n",
    "beta1 = .25\n",
    "\n",
    "def gen_data(beta0=beta0,beta1=beta1):\n",
    "    nobs = 1000\n",
    "    #parameters\n",
    "    sigma = 1\n",
    "    \n",
    "    epsilon = stats.norm.rvs(loc=0,scale=sigma,size=nobs)\n",
    "    #censor data below x<0?\n",
    "    x = stats.norm.rvs(loc=5,scale=5,size=nobs)\n",
    "    y = beta0+ beta1*x + epsilon\n",
    "    \n",
    "    #censor\n",
    "    y[y<=0] = 0\n",
    "    return y,x,nobs\n",
    "\n",
    "\n",
    "yn,xn,nobs = gen_data()\n",
    "print(xn.shape)\n",
    "print(sm.add_constant(xn).shape)\n",
    "print(scipy.stats.mode(yn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed()\n",
    "yn,xn,nobs = gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Tobit Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>             <td>y</td>         <th>  Log-Likelihood:    </th> <td> -1385.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>Tobit</td>       <th>  AIC:               </th> <td>   2774.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>           <td>Maximum Likelihood</td> <th>  BIC:               </th> <td>   2784.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>              <td>Mon, 28 Dec 2020</td>  <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                  <td>18:03:45</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>       <td>  1000</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>           <td>   998</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>               <td>     1</td>       <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    1.0362</td> <td>    0.048</td> <td>   21.692</td> <td> 0.000</td> <td>    0.943</td> <td>    1.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.2439</td> <td>    0.007</td> <td>   34.918</td> <td> 0.000</td> <td>    0.230</td> <td>    0.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>var</th>   <td>    1.0066</td> <td>    0.047</td> <td>   21.343</td> <td> 0.000</td> <td>    0.914</td> <td>    1.099</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                Tobit Results                                 \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   Log-Likelihood:                -1385.2\n",
       "Model:                          Tobit   AIC:                             2774.\n",
       "Method:            Maximum Likelihood   BIC:                             2784.\n",
       "Date:                Mon, 28 Dec 2020                                         \n",
       "Time:                        18:03:45                                         \n",
       "No. Observations:                1000                                         \n",
       "Df Residuals:                     998                                         \n",
       "Df Model:                           1                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          1.0362      0.048     21.692      0.000       0.943       1.130\n",
       "x1             0.2439      0.007     34.918      0.000       0.230       0.258\n",
       "var            1.0066      0.047     21.343      0.000       0.914       1.099\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Tobit(GenericLikelihoodModel):\n",
    "    \n",
    "    def __init__(self, *args,cc=False,ols=False, **kwargs):\n",
    "        super(Tobit,self).__init__(*args,**kwargs)\n",
    "        self._set_extra_params_names(['var'])\n",
    "        self.start_params = np.array([1]*(self.exog.shape[1]+1))\n",
    "        self.cc = cc\n",
    "        self.ols = ols\n",
    "        #self.start_params = np.array( range(1, (2*self.exog.shape[1]+2)))\n",
    "        #2 sets of params for z, 1 for x, 2 variances...\n",
    "    \n",
    "    def loglikeobs(self, params):\n",
    "        y = self.endog\n",
    "        x = self.exog\n",
    "        m = 1*(self.endog == 0) #missingness\n",
    "        \n",
    "        beta = params[0:-1]\n",
    "        sigma2 = max(params[-1],1e-3)\n",
    "        \n",
    "        mu_y = np.matmul(x,beta)\n",
    "        \n",
    "        pr_y = stats.norm.logpdf( y, loc = mu_y, scale=np.sqrt(sigma2))\n",
    "        \n",
    "       \n",
    "        #if complete case, assign pr missing to all observations...\n",
    "        pr_m = np.log(max(m.mean(),1e-4))\n",
    "        if not self.cc:\n",
    "            pr_m = stats.norm.logcdf( y, loc = mu_y, scale=np.sqrt(sigma2))\n",
    "        \n",
    "        #we're done if ols\n",
    "        if self.ols:\n",
    "            return pr_y\n",
    "        else:\n",
    "            ll = (1-m)*pr_y + m*pr_m\n",
    "            return ll\n",
    "        \n",
    "    def score(self, params):\n",
    "        y = self.endog\n",
    "        x = self.exog\n",
    "        m = 1*(self.endog == 0) #missingness\n",
    "        m_x = np.repeat(m,x.shape[1]).reshape(x.shape)\n",
    "        \n",
    "        if ols: #if OLS use all the data...\n",
    "            m, m_x = np.ones(y.shape), np.ones(x.shape)\n",
    "        \n",
    "        \n",
    "        b = params[0:-1]\n",
    "        sigma2 = max(params[-1],1e-3)\n",
    "        s =  np.sqrt(sigma2)\n",
    "\n",
    "        beta_jac = np.zeros(len(b))\n",
    "        sigma_jac = 0\n",
    "        \n",
    "        #for censored\n",
    "        if not cc and not ols: \n",
    "            left_stats = (y - np.dot(x, b)) / s\n",
    "            l_pdf = scipy.stats.norm.logpdf(left_stats)\n",
    "            l_cdf = scipy.stats.norm.logcdf(left_stats)\n",
    "            left_frac = np.exp(l_pdf - l_cdf)\n",
    "            beta_left = np.dot(left_frac*m, x*m_x / s)\n",
    "            beta_jac -= beta_left\n",
    "            left_sigma = np.dot(left_frac*m, left_stats*m)\n",
    "            sigma_jac -= left_sigma\n",
    "        \n",
    "        #for non-censored\n",
    "        mid_stats = (y - np.dot(x, b)) / s\n",
    "        beta_mid = np.dot(mid_stats*(1-m), x*(1-m_x) / s)\n",
    "        beta_jac += beta_mid\n",
    "        mid_sigma = ((np.square(mid_stats) - 1)*(1-m)).sum()\n",
    "        sigma_jac += mid_sigma\n",
    "        \n",
    "        combo_jac = np.append(beta_jac, sigma_jac / (2*s) )  # by chain rule, since the expression above is dloglik/dlogsigma\n",
    "        return combo_jac\n",
    "\n",
    "\n",
    "model1 =  Tobit(yn,sm.add_constant(xn))\n",
    "model1_fit = model1.fit(disp=False)\n",
    "model1_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# playing with eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Power_divergenceResult' object has no attribute 'pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-dbb779a96af9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0myn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mmodel_eigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_eigen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mchisq_pdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchisquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_eigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchisq_pdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Power_divergenceResult' object has no attribute 'pdf'"
     ]
    }
   ],
   "source": [
    "def setup_shi(yn,xn):\n",
    "    model1 = Tobit(yn,sm.add_constant(xn))\n",
    "    model1_fit = model1.fit(disp=False)\n",
    "    ll1 = model1.loglikeobs(model1_fit.params)\n",
    "    grad1 =  model1.score_obs(model1_fit.params)    \n",
    "    hess1 = model1.hessian(model1_fit.params)\n",
    "    k1 = len(model1_fit.params)\n",
    "    \n",
    "    #fit logistic values\n",
    "    model2 = Tobit(yn,sm.add_constant(xn),ols=True)\n",
    "    model2_fit = model2.fit(disp=False)\n",
    "    ll2 = model2.loglikeobs(model2_fit.params)\n",
    "    grad2 =  model2.score_obs(model2_fit.params)    \n",
    "    hess2 = model2.hessian(model2_fit.params)\n",
    "    k2 = len(model2_fit.params)\n",
    "    \n",
    "    return ll1,grad1,hess1,ll2,k1, grad2,hess2,k2\n",
    "\n",
    "\n",
    "def compute_eigen(yn,xn):\n",
    "    ll1,grad1,hess1,ll2,k1, grad2,hess2,k2 = setup_shi(yn,xn)\n",
    "    k = k1 + k2\n",
    "    n = len(ll1)\n",
    "    \n",
    "    #A_hat:\n",
    "    A_hat1 = np.concatenate([hess1,np.zeros((k1,k2))])\n",
    "    A_hat2 = np.concatenate([np.zeros((k2,k1)),-1*hess2])\n",
    "    A_hat = np.concatenate([A_hat1,A_hat2],axis=1)\n",
    "\n",
    "    #B_hat, covariance of the score...\n",
    "    B_hat =  np.concatenate([grad1,-grad2],axis=1) #might be a mistake here..\n",
    "    B_hat = np.cov(B_hat.transpose())\n",
    "    \n",
    "    #compute eigenvalues for weighted chisq\n",
    "    sqrt_B_hat= linalg.sqrtm(B_hat)\n",
    "    W_hat = np.matmul(sqrt_B_hat,linalg.inv(A_hat))\n",
    "    W_hat = np.matmul(W_hat,sqrt_B_hat)\n",
    "    V,W = np.linalg.eig(W_hat)\n",
    "    return np.abs(V)\n",
    "\n",
    "\n",
    "supp = np.arange(-5,5,.1)\n",
    "yn,xn,nobs = gen_data()\n",
    "model_eigs = compute_eigen(yn,xn)\n",
    "normal_draws = stats.normal.rvs( loc=0,scale = model_eigs)\n",
    "#normal_draws = \n",
    "\n",
    "plt.hist(weighted_chi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
